# =============================================================================
# AI-Dev-Sandbox — Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values.
#   cp .env.template .env
#
# API keys set here are written into /workspace/.sandbox.env by the host
# entrypoint on each VM start, and passed into the ai-dev-sandbox container
# inside the guest VM via --env-file.
# =============================================================================

# =============================================================================
# WORKSPACE
# Your project directory — mounted at /workspace inside the VM and the
# ai-dev-sandbox container.
# =============================================================================
HOST_WORKSPACE=./workspace

# =============================================================================
# AI API KEYS
# =============================================================================
ANTHROPIC_API_KEY=
OPENAI_API_KEY=
GOOGLE_API_KEY=
GEMINI_API_KEY=
GITHUB_TOKEN=

# =============================================================================
# OLLAMA (optional — local host LLM on host)
# 172.16.0.1 is the TAP gateway (= Docker host as seen from inside the VM).
# If Ollama is running on your host machine, this default works as-is.
# =============================================================================
#OLLAMA_HOST=http://172.16.0.1:11434
#OLLAMA_MODEL=llama3.2

# =============================================================================
# EGRESS POLICY (optional - local lan ollama server)
# If your LLM runs on another machine in the same network, add its IP to EGRESS_ALLOW_IP.
#EGRESS_ALLOW_IP=http target is still the same IP, ports stay numeric
#EGRESS_ALLOW_IP=192.168.10.50
#EGRESS_ALLOW_TCP_PORTS=11434

# =============================================================================
# VOICE MODE (optional)
# =============================================================================
WHISPER_MODEL=base.en
PIPER_VOICE=en_US-amy-medium
VOICE_AI_TOOL=claude

# =============================================================================
# VM RESOURCES
# Sized for running Docker + AI tools comfortably.
# =============================================================================

# vCPUs for the Firecracker MicroVM
FC_VCPU=8

# RAM in MB
FC_MEM=8192

# Workspace block device size in MB (how much disk the VM sees at /workspace)
FC_WORKSPACE_SIZE=4096

# Firecracker log level: Error | Warning | Info | Debug
FC_LOG_LEVEL=Warning

# Console mode: interactive (attach to serial console) | detached (run in bg)
FC_CONSOLE_TYPE=interactive
