# =============================================================================
# AI-Dev-Sandbox — Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values.
#   cp .env.template .env
#
# API keys set here are written into /workspace/.sandbox.env by the host
# entrypoint on each VM start, and passed into the ai-dev-sandbox container
# inside the guest VM via --env-file.
# =============================================================================

# =============================================================================
# WORKSPACE
# Your project directory — mounted at /workspace inside the VM and the
# ai-dev-sandbox container.
# =============================================================================
HOST_WORKSPACE=./workspace

# =============================================================================
# AI API KEYS
# =============================================================================
# Anthropic Claude API Key
# Get your key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# OpenAI API Key (optional)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# To Do: which variable and which site?
# Google Gemini API Key (optional)
# Get your key from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=

# https://aistudio.google.com/api-keys
GEMINI_API_KEY=

GITHUB_TOKEN=



# =============================================================================
# OLLAMA (optional — local host LLM on host)
# 172.16.0.1 is the TAP gateway (= Docker host as seen from inside the VM).
# If Ollama is running on your host machine, this default works as-is.
# If it is on another server then update the ip address.
# =============================================================================
OLLAMA_HOST=http://172.16.0.1:11434
OLLAMA_MODEL=llama3.2:latest

# =============================================================================
# EGRESS POLICY (optional - local lan ollama server)
# If your LLM runs on another machine in the same network, add its IP to EGRESS_ALLOW_IP.
#EGRESS_ALLOW_IP=http target is still the same IP, ports stay numeric
#EGRESS_ALLOW_IP=192.168.10.50
#EGRESS_ALLOW_TCP_PORTS=11434

# =============================================================================
# VOICE MODE (optional)
# To enable voice, run on host:
#   pactl load-module module-native-protocol-tcp auth-ip-acl="127.0.0.1;172.16.0.0/24" auth-anonymous=1
# =============================================================================
WHISPER_MODEL=base.en
PIPER_VOICE=en_US-amy-medium
VOICE_AI_TOOL=claude
#PULSE_SERVER=tcp:172.16.0.1:4713

# =============================================================================
# VM RESOURCES
# Sized for running Docker + AI tools comfortably.
# =============================================================================

# vCPUs for the Firecracker MicroVM
FC_VCPU=8

# RAM in MB
FC_MEM=8192

# Workspace block device size in MB (how much disk the VM sees at /workspace)
FC_WORKSPACE_SIZE=4096

# Firecracker log level: Error | Warning | Info | Debug
FC_LOG_LEVEL=Warning

# Console mode: interactive (attach to serial console) | detached (run in bg)
FC_CONSOLE_TYPE=interactive
